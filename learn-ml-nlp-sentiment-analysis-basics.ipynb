{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# X sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# for using regular expressiion\n",
    "import re\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#for plotting seaborn plots\n",
    "import seaborn as sns\n",
    "\n",
    "#The string module contains a number of useful constants and classes\n",
    "import string\n",
    "\n",
    "#NLTK is a leading platform for building Python programs to work \n",
    "#with human language data. It provides easy-to-use interfaces to \n",
    "#over 50 corpora and lexical resources such as WordNet, \n",
    "#along with a suite of text processing libraries for \n",
    "#classification, tokenization, stemming, tagging, parsing, \n",
    "#and semantic reasoning, wrappers for industrial-strength NLP \n",
    "#libraries, and an active discussion forum.\n",
    "\n",
    "import nltk\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#When using the 'inline' backend, so that matplotlib graphs will be included in notebook,\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66a6bd91342941f2003ffd8ecf01bb3b030ad4f1"
   },
   "source": [
    "### loading data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8238b0304bb96f515cd8fbb1918ea9f7a1c20860"
   },
   "outputs": [],
   "source": [
    "train  = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c0008ef457084a783b079a526eae0dac8e19dee"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "209b42adbd60b7ee556ab03bd1efd274f5d63766"
   },
   "source": [
    "### For convenience only, let’s first combine train and test set. \n",
    "\n",
    "### This saves the trouble of performing the same steps twice on test and train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe789fb912d22ae5123d37c15ca944d436330ed0"
   },
   "outputs": [],
   "source": [
    "combi = train.append(test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a66b89c4689f091edab0b44afb56aa97eeb13490"
   },
   "source": [
    "\n",
    "so the data has 3 columns id, label, and tweet. \n",
    "\n",
    "label is the binary target variable and tweet contains the tweets that is to be  cleaned and preprocessed\n",
    "\n",
    "so we will work on \n",
    "\n",
    "1.Removing Twitter Handles (@user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b34f38e078741fa589515271435b3cd11e1c03e"
   },
   "source": [
    "## Removing Twitter Handles (@user)\n",
    "### we are going to use regular expression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47c4e59a236529928d4b3d9f8c7d4862e1554fe2"
   },
   "outputs": [],
   "source": [
    "#a user-defined function to remove unwanted text patterns from the tweets.\n",
    "#inputs -- two arguments, one is the original string of text and the other is the pattern of text that we want to remove from the string.\n",
    "#outputs - -returns the same input string but without the given pattern. We will use this function to remove the pattern ‘@user’ from all the tweets in our data.\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i,'', input_txt)\n",
    "        \n",
    "    return input_txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1194d9f25d3c43006a3990367f336f2a441e4e8b"
   },
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9511f509fe2ae7c48ce4e4cb30ff8fff6747caad"
   },
   "outputs": [],
   "source": [
    "# checking the changes in the data\n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4a33a2aa1ea20e702c01cd76c09172264c22eb2"
   },
   "source": [
    "### Removing Punctuations, Numbers, and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ead60f6dbea78eb098cddde3c2779f5d5c5f8fc4"
   },
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65443b9f3fd1f17d4fcd61c62fc81609eff5e772"
   },
   "source": [
    "### removing short words (0f <3 characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7be69fdbe01a673f14faa8e2ad28982da755b59"
   },
   "outputs": [],
   "source": [
    "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3eb0b5e3e8cdefb21c74da122822c6bcb30b2ac2"
   },
   "outputs": [],
   "source": [
    "# lets see the changes in the dataset\n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf75395d82080677a8444c9643f469dbd3f0cf31"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "tokenize all the cleaned tweets in our dataset.\n",
    "\n",
    "Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d52f579203fba3c7c72112f2e1e949264794c1c6"
   },
   "outputs": [],
   "source": [
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77237c42e72eeb25940bc023ec8fd8b505c770a6"
   },
   "source": [
    "### Stemming\n",
    "\n",
    "\n",
    "Stemming is a rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3fa980943bdf1974acc263a8b1ecad171b37160b"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])    # stemming\n",
    "\n",
    "\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "347030e4f1fae46d1aa2c6d4b31dfaf9932115c5"
   },
   "outputs": [],
   "source": [
    "#snitch these tokens back togather\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "\n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc0e5ca8e06b8ae3e9278d28aea0562d21215800"
   },
   "source": [
    "## Story generation and visualization from tweets\n",
    "try to answer \n",
    "\n",
    "these questions\n",
    "\n",
    "\n",
    "What are the most common words in the entire dataset?\n",
    "\n",
    "\n",
    "What are the most common words in the dataset for negative and positive tweets, respectively?\n",
    "\n",
    "\n",
    "How many hashtags are there in a tweet?\n",
    "\n",
    "\n",
    "Which trends are associated with my dataset?\n",
    "\n",
    "\n",
    "Which trends are associated with either of the sentiments? Are they compatible with the sentiments?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9db875f0caa99f178224e4c115cd0664e44aa951"
   },
   "source": [
    "### A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28db2ca78453aeddca2bf440852dfbe1a2929ecc"
   },
   "outputs": [],
   "source": [
    "# common words\n",
    "\n",
    "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2ce447664749a7facdea9df94e644d4c6b17dba"
   },
   "outputs": [],
   "source": [
    "## non resisit or normal words\n",
    "\n",
    "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2a1a4355c19dc396b588bcb5b6657e33f8b3d70"
   },
   "outputs": [],
   "source": [
    "# rasisit or negative words\n",
    "\n",
    "negative_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
    "\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c59c446d5bc443ba64b6c28573c85e880a9d905f"
   },
   "source": [
    "### impact of hashtag on tweets sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c82ef3316a7e51d0227edbe444cb5509de19a934"
   },
   "outputs": [],
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c77d24d77424f7b663462d886a841fa0c406345"
   },
   "outputs": [],
   "source": [
    "\n",
    "# extracting hashtags from non racist/sexist tweets\n",
    "\n",
    "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n",
    "\n",
    "# extracting hashtags from racist/sexist tweets\n",
    "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n",
    "\n",
    "# unnesting list\n",
    "HT_regular = sum(HT_regular,[])\n",
    "HT_negative = sum(HT_negative,[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bdd72bfdac73bf8ffd193499abba7a71c7dc884"
   },
   "source": [
    "### plotting positive hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65928e090b49b46975637cbe85935b31b7edc507"
   },
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(HT_regular)\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "\n",
    "# selecting top 20 most frequent hashtags     \n",
    "d = d.nlargest(columns=\"Count\", n = 20) \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b023e0f023ec4dd259ec0528efe9235203ef5de"
   },
   "source": [
    "### plotting negative hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1edb74953a2afd32eab8143b52db7db7c814d449"
   },
   "outputs": [],
   "source": [
    "b = nltk.FreqDist(HT_negative)\n",
    "e = pd.DataFrame({'Hashtag': list(a.keys()),\n",
    "                  'Count': list(a.values())})\n",
    "# selecting top 10 most frequent hashtags   \n",
    "\n",
    "\n",
    "e = e.nlargest(columns=\"Count\", n = 10) \n",
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50340938a66421c01c2611b71e556ca7cf2ee5b3"
   },
   "source": [
    "## Extracting features fromm cleaned words\n",
    "To analyze a preprocessed data, it needs to be converted into features.\n",
    "\n",
    "\n",
    "Depending upon the usage, text features can be constructed using assorted \n",
    "\n",
    "\n",
    "techniques – Bag-of-Words, TF-IDF, and Word Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7864aa12a45870d618d7090d2688e809a781e06d"
   },
   "outputs": [],
   "source": [
    "# Applying Bag-of-words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b7bd73610fcca6458d5994bb1f81b9b3d6746ae"
   },
   "outputs": [],
   "source": [
    "# Applying TF-IDF \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc51e77c12df94dfdfd8c05ef5b250b165500f5c"
   },
   "source": [
    "## Model Building: Sentiment Analysis\n",
    "\n",
    "\n",
    "### using Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44ca35ead4bb5c8753da2c585e8cf742532ca190"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_bow = bow[:31962,:]\n",
    "test_bow = bow[31962:,:]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain_bow, ytrain) # training the model\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
    "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int) # calculating f1 score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1d76a04e9f2c490e9563a02784ad782f24229c2"
   },
   "source": [
    "### using IF-IDF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df231445efa8334c863c66a36d15244fb59abece"
   },
   "outputs": [],
   "source": [
    "# Building model using TF-IDF features\n",
    "train_tfidf = tfidf[:31962,:]\n",
    "test_tfidf = tfidf[31962:,:]\n",
    "\n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "lreg.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "078e8fb86c45d50d0cb8af432d78c1bdbb9bfec2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afe1f9ada4689c8bde89404e1e0c6ed95fe58abb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
